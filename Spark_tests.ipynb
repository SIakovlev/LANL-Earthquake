{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T04:25:43.245848Z",
     "start_time": "2019-03-01T04:25:41.023998Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.context import SparkContext\n",
    "# SparkContext.setSystemProperty('spark.executor.memory', '8g')\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T04:27:16.073869Z",
     "start_time": "2019-03-01T04:27:16.064312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x10ee016a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.setAll([('spark.executor.memory', '8g'), \n",
    "                           ('spark.app.name', 'Spark Updated Conf'), \n",
    "                           ('spark.executor.cores', '8'), \n",
    "                           ('spark.cores.max', '6'), \n",
    "                           ('spark.driver.memory','8g')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T04:27:17.995443Z",
     "start_time": "2019-03-01T04:27:17.981245Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.host', '10.13.68.243'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.id', 'local-1551414342977'),\n",
       " ('spark.app.name', 'Spark Updated Conf'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.memory', '8g'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.memory', '8g'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.port', '53129'),\n",
       " ('spark.executor.cores', '8'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.cores.max', '6')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example with Spark to check that everything is working\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 100000000\n",
    "\n",
    "def inside(p):     \n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we'll train a linear regressor using all data we have\n",
    "\n",
    "import csv\n",
    "csv_file = '/Users/sergey/Dev/Kaggle/LANL-Earthquake-Prediction/train.csv'\n",
    "txt_file = '/Users/sergey/Dev/Kaggle/LANL-Earthquake-Prediction/train.txt'\n",
    "with open(txt_file, \"w\") as my_output_file:\n",
    "    with open(csv_file, \"r\") as my_input_file:\n",
    "        [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "    my_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD, LinearRegressionModel\n",
    "\n",
    "# Load and parse the data\n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(' ')]\n",
    "    return LabeledPoint(values[1], [values[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile(\"/Users/sergey/Dev/Kaggle/LANL-Earthquake-Prediction/train.txt\")\n",
    "content = data.zipWithIndex().filter(lambda kv: kv[1] > 1).keys()\n",
    "parsedData = content.map(parsePoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at the data\n",
    "parsedData.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = LinearRegressionWithSGD.train(parsedData, iterations=100, step=0.00000001)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "valuesAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "MSE = valuesAndPreds \\\n",
    "    .map(lambda vp: (vp[0] - vp[1])**2) \\\n",
    "    .reduce(lambda x, y: x + y) / valuesAndPreds.count()\n",
    "print(\"Mean Squared Error = \" + str(MSE))\n",
    "\n",
    "# Save and load model\n",
    "model.save(sc, \"/Users/sergey/Dev/Kaggle/LANL-Earthquake-Prediction/models/pythonLinearRegressionWithSGDModel\")\n",
    "sameModel = LinearRegressionModel.load(sc, \"/Users/sergey/Dev/Kaggle/LANL-Earthquake-Prediction/models/pythonLinearRegressionWithSGDModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "days = lambda i: i * 86400\n",
    "\n",
    "\n",
    "# cSchema = StructType([StructField(\"WordList\", ArrayType(StringType()))])\n",
    "\n",
    "# notice extra square brackets around each element of list \n",
    "# test_list = [['Hello', 'world']], [['I', 'am', 'fine']]\n",
    "\n",
    "# df = spark.createDataFrame(data=test_list, schema=cSchema) \n",
    "\n",
    "df = sqlContext.createDataFrame([(17, \"2017-03-10T15:27:18+00:00\"),\n",
    "                        (13, \"2017-03-15T12:27:18+00:00\"),\n",
    "                        (25, \"2017-03-18T11:27:18+00:00\")],\n",
    "                        [\"dollars\", \"timestampGMT\"])\n",
    "\n",
    "df = df.withColumn('timestampGMT', df.timestampGMT.cast('timestamp'))\n",
    "\n",
    "#create window by casting timestamp to long (number of seconds)\n",
    "w = (Window.orderBy(F.col(\"timestampGMT\").cast('long')).rangeBetween(-days(7), 0))\n",
    "\n",
    "df = df.withColumn('rolling_average', F.avg(\"dollars\").over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "data = [\n",
    "    LabeledPoint(0.0, [0.0]),\n",
    "    LabeledPoint(1.0, [1.0]),\n",
    "    LabeledPoint(1.0, [2.0]),\n",
    "    LabeledPoint(1.0, [3.0])\n",
    "]\n",
    "\n",
    "svm = SVMWithSGD.train(sc.parallelize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.format(\"csv\").options(header=\"true\", inferschema='true').load(\"/Users/sergey/Dev/Kaggle/LANL-Earthquake-Prediction/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler(inputCols = ['acoustic_data'], outputCol = 'features')\n",
    "v_df = vectorAssembler.transform(df)\n",
    "v_df = v_df.select(['features', 'time_to_failure'])\n",
    "v_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = v_df.randomSplit([0.7, 0.3])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "# lr = LinearRegressionWithSGD.train(sc.parallelize(v_df), iterations=10)\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='time_to_failure', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(' ')]\n",
    "    return LabeledPoint(values[0], values[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrm = LinearRegressionWithSGD.train(sc.parallelize(rdd_df.collect()), iterations=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
