{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T04:38:04.309301Z",
     "start_time": "2019-02-26T04:38:04.022461Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"Pi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100000000\n",
    "def inside(p):     \n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at this: [link](https://stackoverflow.com/questions/32362783/how-to-change-sparkcontext-properties-in-interactive-pyspark-session):\n",
    "\n",
    "```python\n",
    "conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '4g'), ('spark.app.name', 'Spark Updated Conf'), ('spark.executor.cores', '4'), ('spark.cores.max', '4'), ('spark.driver.memory','4g')])\n",
    "```\n",
    "\n",
    "or \n",
    "\n",
    "\n",
    "```python\n",
    "SparkContext.setSystemProperty('spark.driver.maxResultSize', '10g')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T04:38:08.680481Z",
     "start_time": "2019-02-26T04:38:05.339661Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.context import SparkContext\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '16g')\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "days = lambda i: i * 86400\n",
    "\n",
    "\n",
    "# cSchema = StructType([StructField(\"WordList\", ArrayType(StringType()))])\n",
    "\n",
    "# notice extra square brackets around each element of list \n",
    "# test_list = [['Hello', 'world']], [['I', 'am', 'fine']]\n",
    "\n",
    "# df = spark.createDataFrame(data=test_list, schema=cSchema) \n",
    "\n",
    "df = sqlContext.createDataFrame([(17, \"2017-03-10T15:27:18+00:00\"),\n",
    "                        (13, \"2017-03-15T12:27:18+00:00\"),\n",
    "                        (25, \"2017-03-18T11:27:18+00:00\")],\n",
    "                        [\"dollars\", \"timestampGMT\"])\n",
    "\n",
    "df = df.withColumn('timestampGMT', df.timestampGMT.cast('timestamp'))\n",
    "\n",
    "#create window by casting timestamp to long (number of seconds)\n",
    "w = (Window.orderBy(F.col(\"timestampGMT\").cast('long')).rangeBetween(-days(7), 0))\n",
    "\n",
    "df = df.withColumn('rolling_average', F.avg(\"dollars\").over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "data = [\n",
    "    LabeledPoint(0.0, [0.0]),\n",
    "    LabeledPoint(1.0, [1.0]),\n",
    "    LabeledPoint(1.0, [2.0]),\n",
    "    LabeledPoint(1.0, [3.0])\n",
    "]\n",
    "\n",
    "svm = SVMWithSGD.train(sc.parallelize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "csv_file = '/Users/sergey/Dev/Kaggle/LANL-Earthquake-Prediction/train.csv'\n",
    "txt_file = '/Users/sergey/Dev/Kaggle/LANL-Earthquake-Prediction/train.txt'\n",
    "with open(txt_file, \"w\") as my_output_file:\n",
    "    with open(csv_file, \"r\") as my_input_file:\n",
    "        [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "    my_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T04:56:57.006066Z",
     "start_time": "2019-02-26T04:56:57.003117Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD, LinearRegressionModel\n",
    "\n",
    "# Load and parse the data\n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(' ')]\n",
    "    return LabeledPoint(values[1], [values[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T04:59:30.151733Z",
     "start_time": "2019-02-26T04:56:58.947301Z"
    }
   },
   "outputs": [],
   "source": [
    "data = sc.textFile(\"/Users/sergey/Dev/Kaggle/LANL-Earthquake-Prediction/train.txt\")\n",
    "content = data.zipWithIndex().filter(lambda kv: kv[1] > 1).keys()\n",
    "parsedData = content.map(parsePoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T04:59:47.003979Z",
     "start_time": "2019-02-26T04:59:46.777435Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.4690999821, [6.0]),\n",
       " LabeledPoint(1.469099981, [8.0]),\n",
       " LabeledPoint(1.4690999799, [5.0])]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedData.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T06:22:30.169812Z",
     "start_time": "2019-02-26T04:59:57.751329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 45.73167411668977\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = LinearRegressionWithSGD.train(parsedData, iterations=100, step=0.00000001)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "valuesAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "MSE = valuesAndPreds \\\n",
    "    .map(lambda vp: (vp[0] - vp[1])**2) \\\n",
    "    .reduce(lambda x, y: x + y) / valuesAndPreds.count()\n",
    "print(\"Mean Squared Error = \" + str(MSE))\n",
    "\n",
    "# Save and load model\n",
    "model.save(sc, \"/Users/sergey/Dev/Kaggle/LANL-Earthquake-Prediction/models/pythonLinearRegressionWithSGDModel\")\n",
    "sameModel = LinearRegressionModel.load(sc, \"/Users/sergey/Dev/Kaggle/LANL-Earthquake-Prediction/models/pythonLinearRegressionWithSGDModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.format(\"csv\").options(header=\"true\", inferschema='true').load(\"/Users/sergey/Dev/Kaggle/LANL-Earthquake-Prediction/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler(inputCols = ['acoustic_data'], outputCol = 'features')\n",
    "v_df = vectorAssembler.transform(df)\n",
    "v_df = v_df.select(['features', 'time_to_failure'])\n",
    "v_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = v_df.randomSplit([0.7, 0.3])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "# lr = LinearRegressionWithSGD.train(sc.parallelize(v_df), iterations=10)\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='time_to_failure', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(' ')]\n",
    "    return LabeledPoint(values[0], values[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrm = LinearRegressionWithSGD.train(sc.parallelize(rdd_df.collect()), iterations=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
