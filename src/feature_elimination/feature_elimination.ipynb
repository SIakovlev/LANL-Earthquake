{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/pavel/Documents/0Research/Projects/LANL-Earthquake/src/data_processing')\n",
    "from dp_utils import *\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "data_path = '/home/pavel/Documents/0Research/Projects/LANL-Earthquake/data/EQs/EQ_2.h5'\n",
    "df = pd.read_hdf(data_path, key = 'table')\n",
    "df.columns = ['s','ttf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_abs_energy(s, window_size=5000): 100%|██████████| 8886/8886 [00:00<00:00, 16840.63it/s]\n",
      "\t window decorator: \n",
      "\t - window size: 5000\n",
      "w_absolute_sum_of_changes(s, window_size=15000): 100%|██████████| 2962/2962 [00:00<00:00, 9566.04it/s]\n",
      "\t window decorator: \n",
      "\t - window size: 15000\n",
      "w_autocorrelation(s, window_size=10000, lag=100): 100%|██████████| 4443/4443 [00:01<00:00, 3525.75it/s]\n",
      "\t window decorator: \n",
      "\t - window size: 10000\n",
      "w_binned_entropy(w_absolute_sum_of_changes(s, window_size=15000), window_size=2, max_bins=10): 100%|██████████| 1481/1481 [00:00<00:00, 5108.16it/s]\n",
      "\t window decorator: \n",
      "\t - window size: 2\n",
      "w_c3(s, window_size=10000, lag=100): 100%|██████████| 4443/4443 [00:00<00:00, 8060.37it/s]\n",
      "\t window decorator: \n",
      "\t - window size: 10000\n",
      "w_psd(s, window_size=10000, fs=4000000.0): 100%|██████████| 4443/4443 [00:02<00:00, 2174.89it/s]\n",
      "\t window decorator: \n",
      "\t - window size: 10000\n",
      "w_savgol_filter(w_psd(s, window_size=10000, fs=4000000.0), window_size=9999999999, window_length=201, polyorder=1):\n",
      "\t window decorator: \n",
      "\t - window size: 9999999999\n",
      "ttf: 100%|██████████| 4443/4443 [00:00<00:00, 19708.35it/s]\n",
      "\t window decorator: \n",
      "\t - window size: 10000\n",
      "downsample w_abs_energy(s, window_size=5000) from 8886 to size 4443\n",
      "upsample w_absolute_sum_of_changes(s, window_size=15000) from 2962 to size 4443\n",
      "upsample w_binned_entropy(w_absolute_sum_of_changes(s, window_size=15000), window_size=2, max_bins=10) from 1481 to size 4443\n"
     ]
    }
   ],
   "source": [
    "config_path = \"/home/pavel/Documents/0Research/Projects/LANL-Earthquake/src/data_processing/dp_config.json\"\n",
    "with open(config_path, 'rb') as config:\n",
    "    params = json.load(config)\n",
    "\n",
    "routines = params['routines']\n",
    "default_window_size = params['window_size']\n",
    "new_df = process_df(df, routines,default_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to = '/home/pavel/Documents/0Research/Projects/LANL-Earthquake/data/featurized_data_EQ_2.h5'\n",
    "new_df.to_hdf(save_to, key = 'table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X = new_df.loc[:, new_df.columns != 'ttf']\n",
    "X = np.array(X)\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "y = np.array(new_df.ttf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing features with low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "было:  8\n",
      "стало:  6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "print(\"было: \",len(new_df.columns))\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "selected_features = sel.fit_transform(new_df)\n",
    "print(\"стало: \",selected_features.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate feature selection works by selecting the best features based on univariate statistical tests. It can be seen as a preprocessing step to an estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SelectKBest** removes all but the  highest scoring features.\n",
    "\n",
    "The methods based on **F-test** estimate the degree of linear dependency between two random variables. On the other hand, **mutual information** methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4443, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest,SelectFpr, SelectFdr\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "X_new = SelectKBest(mutual_info_regression, k=3).fit_transform(X, y)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SelectFpr**\n",
    "\n",
    "Filter: Select the pvalues below alpha based on a FPR test.\n",
    "\n",
    "FPR test stands for False Positive Rate test. It controls the total amount of false detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4443, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pavel/.local/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:298: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= X_norms\n",
      "/home/pavel/.local/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/home/pavel/.local/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/home/pavel/.local/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1821: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "/home/pavel/.local/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:560: RuntimeWarning: invalid value encountered in less\n",
      "  return self.pvalues_ < self.alpha\n"
     ]
    }
   ],
   "source": [
    "X_new = SelectFpr(f_regression,alpha = 0.1).fit_transform(X, y)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SelectFdr**\n",
    "\n",
    "Filter: Select the p-values for an estimated false discovery rate\n",
    "\n",
    "This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4443, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pavel/.local/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:619: RuntimeWarning: invalid value encountered in less_equal\n",
      "  np.arange(1, n_features + 1)]\n",
      "/home/pavel/.local/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:622: RuntimeWarning: invalid value encountered in less_equal\n",
      "  return self.pvalues_ <= selected.max()\n"
     ]
    }
   ],
   "source": [
    "X_new = SelectFdr(f_regression,alpha = 0.1).fit_transform(X, y)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive feature elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (**RFE**) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeRegressor as DTR\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "hasattr(sklearn,'tree.DecisionTreeRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True False False False False  True]\n",
      "Remaining features: \n",
      "['w_abs_energy(s, window_size=5000)', 'w_absolute_sum_of_changes(s, window_size=15000)', 'w_savgol_filter(w_psd(s, window_size=10000, fs=4000000.0), window_size=9999999999, window_length=201, polyorder=1)']\n",
      "[1 1 2 5 3 4 1]\n"
     ]
    }
   ],
   "source": [
    "estimator = DTR()\n",
    "selector = RFE(estimator, 3, step=1)\n",
    "\n",
    "selector = selector.fit(X, y)\n",
    "print(selector.support_) \n",
    "print(\"Remaining features: \")\n",
    "print([col for indicator, col in zip(selector.support_,list(new_df.columns)) if indicator == True])\n",
    "print(selector.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFE(estimator=DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best'),\n",
       "  n_features_to_select=3, step=1, verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True False False False False  True]\n",
      "Remaining features: \n",
      "['w_abs_energy(df, window_size=5000)', 'w_absolute_sum_of_changes(df, window_size=15000)', 'w_savgol_filter(df, window_size=9999999999, window_length=201, polyorder=1)']\n",
      "[1 1 3 5 2 4 1]\n"
     ]
    }
   ],
   "source": [
    "estimator = RFR()\n",
    "selector = RFE(estimator, 3, step=1)\n",
    "\n",
    "selector = selector.fit(X, y)\n",
    "print(selector.support_) \n",
    "print(\"Remaining features: \")\n",
    "print([col for indicator, col in zip(selector.support_,list(new_df.columns)) if indicator == True])\n",
    "print(selector.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1-based feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear models penalized with the L1 norm have sparse solutions: many of their estimated coefficients are zero. When the goal is to reduce the dimensionality of the data to use with another classifier, they can be used along with feature_selection.SelectFromModel to select the non-zero coefficients. In particular, sparse estimators useful for this purpose are the linear_model.Lasso for regression, and of linear_model.LogisticRegression and svm.LinearSVC for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4443, 7)\n",
      "(4443, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "print(X.shape)\n",
    "regressor = Lasso(alpha = 0.0015, normalize = True).fit(X, y)\n",
    "model = SelectFromModel(regressor, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-based feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor, ExtraTreesRegressor\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4443, 7)\n",
      "[0.16389555 0.35380295 0.11044488 0.         0.10723329 0.12514752\n",
      " 0.13947582]\n",
      "(4443, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "clf = ExtraTreesRegressor(n_estimators=50)\n",
    "clf = clf.fit(X, y)\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "print(clf.feature_importances_)\n",
    "X_new = model.transform(X)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
