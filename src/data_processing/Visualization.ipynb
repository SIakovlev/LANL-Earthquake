{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "import plotly\n",
    "from EQ_chunking import chunk_data_on_EQs\n",
    "import os, fnmatch\n",
    "plotly.tools.set_credentials_file(username='ptolmachev', api_key='Fs5sBFAg7YuBn52rzy6n')\n",
    "from dp_utils import *\n",
    "\n",
    "def nice_plot(series):\n",
    "    fig = plt.figure(figsize = (16,4))\n",
    "    plt.grid(True)\n",
    "    try:\n",
    "        plt.plot(series.compute().tolist(), 'r-',linewidth = 2, alpha = 0.7)\n",
    "    except:\n",
    "        plt.plot(series.tolist(), 'r-',linewidth = 2, alpha = 0.7)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dir_contains_EQs(data_path, EQs_num):\n",
    "    list_dir = os.listdir(data_path)\n",
    "    for i in range(len(list_dir)):\n",
    "        subdir_path = data_path + \"/\" + list_dir[i]\n",
    "        if os.path.isdir(subdir_path):\n",
    "            if all([os.path.isfile(subdir_path + \"/\" + \"EQ_\" + str(num) + \".h5\") for num in EQs_num]):\n",
    "                return subdir_path\n",
    "    return None\n",
    "\n",
    "def find(pattern, path):\n",
    "    result = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for name in files:\n",
    "            if fnmatch.fnmatch(name, pattern):\n",
    "                result.append(os.path.join(root, name))\n",
    "    if len(result) == 0:\n",
    "        return None\n",
    "    return result[0]\n",
    "            \n",
    "\n",
    "def plot_data(function, params, **kwargs):\n",
    "    size_of_slice = 150000\n",
    "    data_path = kwargs['path_to_data']\n",
    "    list_of_EQs_to_plot = kwargs['EQs_num']\n",
    "    \n",
    "    #Preparation part (if you haven't done chunking yourself it does it for you)\n",
    "    EQ_dir = find_dir_contains_EQs(data_path, list_of_EQs_to_plot)\n",
    "    if EQ_dir is None:\n",
    "        print(\"Hold on for a sec...Need to split the data into the chunks first! \\n\")\n",
    "        save_chunks_to = data_path + \"/EQs\"\n",
    "        print(\"Splitting the data, and saving it to {}\\n\".format(save_chunks_to))\n",
    "        full_data_file = find('train.h5', data_path)\n",
    "        if full_data_file is None:\n",
    "            full_data_file = find('train.csv', data_path)\n",
    "        if full_data_file is None:\n",
    "            raise IOError(\"No train dataset on you computer! Please download it prior to runnin g this function.\")\n",
    "        \n",
    "        chunk_data_on_EQs(full_data_file, save_chunks_to)\n",
    "    \n",
    "        print(\"Finished splitting the data.\")\n",
    "    \n",
    "    \n",
    "    # plotting part\n",
    "    dataframes = [pd.read_hdf(data_path + \"/EQs/EQ_\" + str(EQ) + \".h5\", key = 'table') for EQ in list_of_EQs_to_plot]\n",
    "    for i in range(len(dataframes)):\n",
    "        dataframes[i].columns = [\"s\",\"ttf\"]\n",
    "    df = pd.concat(dataframes)\n",
    "    df.columns = [\"s\",\"ttf\"]\n",
    "    names = [function.__name__, \"downsampled_signal\", 'ttf']\n",
    "    \n",
    "    # Downsampling is conducted by the last element      \n",
    "    signals = [function(df.s, **params).values.ravel(), #featurised signal\n",
    "           w_labels(df.s, **params).values.ravel(), #downsampled signal\n",
    "           100*w_labels(df.ttf, **params).values.ravel()] # downsample ttf\n",
    "          \n",
    "    s_max = []\n",
    "    for i in range(len(signals)):\n",
    "        s_max.append(abs(signals[i]).max())\n",
    "    s_max.append(0)\n",
    "\n",
    "    \n",
    "    data1 = [go.Scatter(y=(signals[i] - 0.7*sum(s_max[:i+1])), opacity = 0.7, name  = names[i]) for i in range(len(signals))]\n",
    "\n",
    "    layout1 = dict(\n",
    "        title='Eathquakes ' + function.__name__\n",
    "    )\n",
    "\n",
    "    fig1 = dict(data=data1, layout=layout1)\n",
    "    plotly.offline.plot(fig1, filename = \"Earthquakes.html\", auto_open=True)\n",
    "\n",
    "    #####################################################################################################\n",
    "    # takes the first of specified EQs and plots data related to first, middle and last 150000 samples\n",
    "    b = int( (len(dataframes[0].s) - size_of_slice) / 2)\n",
    "    e = int( (len(dataframes[0].s) + size_of_slice) / 2)\n",
    "    \n",
    "    samples = [function(dataframes[0].s[:size_of_slice], **params).values.ravel(),\n",
    "               function(dataframes[0].s[b:e], **params).values.ravel(),\n",
    "               function(dataframes[0].s[len(dataframes[0])-size_of_slice:], **params).values.ravel()]\n",
    "\n",
    "    s_max = []\n",
    "    for i in range(len(samples)):\n",
    "        s_max.append(abs(samples[i]).max())\n",
    "    s_max.append(0)\n",
    "    \n",
    "    names = [\"first \" + str(size_of_slice),\"middle \"  + str(size_of_slice),\"last \" + str(size_of_slice)]\n",
    "    data2 = [go.Scatter(y= (samples[i] - 0.7*sum(s_max[:i+1])), opacity = 0.7, name  = names[i]) for i in range(len(samples))]\n",
    "\n",
    "    layout2 = dict(\n",
    "        title='Earthquakes ' + function.__name__\n",
    "    )\n",
    "\n",
    "    fig2 = dict(data=data2, layout=layout2)\n",
    "    plotly.offline.plot(fig2, filename = \"Earthquakes samples (first, middle, last \"+ str(size_of_slice) + \" dp).html\", auto_open=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_std(df, window_size=1000): 100%|██████████| 99021/99021 [00:14<00:00, 6931.87it/s]\n",
      "\t window decorator: \n",
      "\t - window size: 1000\n",
      "w_labels(df, window_size=1000): 100%|██████████| 99021/99021 [00:06<00:00, 14145.88it/s]\n",
      "\t window decorator: \n",
      "\t - window size: 1000\n",
      "w_labels(df, window_size=1000): 100%|██████████| 99021/99021 [00:06<00:00, 14955.61it/s]\n",
      "\t window decorator: \n",
      "\t - window size: 1000\n",
      "w_std(df, window_size=1000): 100%|██████████| 150/150 [00:00<00:00, 4687.07it/s]\n",
      "\t window decorator: \n",
      "\t - window size: 1000\n",
      "w_std(df, window_size=1000): 100%|██████████| 150/150 [00:00<00:00, 5833.69it/s]\n",
      "\t window decorator: \n",
      "\t - window size: 1000\n",
      "w_std(df, window_size=1000): 100%|██████████| 150/150 [00:00<00:00, 6583.78it/s]\n",
      "\t window decorator: \n",
      "\t - window size: 1000\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "params = {\"window_size\" : 1000}\n",
    "# One has to specify either: 1) the path to the directory which contains the train.h5 or train.csv file\n",
    "# to do the chunking the program will split the dataset into separate chunks and save them in\n",
    "# the \"specified_path/EQs/\"\n",
    "\n",
    "# or 2) (if you have done the chunking prior to running this code) specify the directory which contains \n",
    "# \"EQs\" folder with chunks in it\n",
    "\n",
    "plot_data(w_std, params, path_to_data=\"/home/pavel/Documents/0Research/Projects/LANL-Earthquake/data\", EQs_num = [2,3] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
